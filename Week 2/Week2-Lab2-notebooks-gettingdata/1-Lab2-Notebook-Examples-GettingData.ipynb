{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Data\n",
    "This notebook showcases how to download data available on the Internet. We cover most formats the data is typically available in, and learn/practice via example Python code or utilities for getting data. \n",
    "\n",
    "TOPIC1: Getting data from a Web URL: text, HTML, XML, PDF.\n",
    "\n",
    "TOPIC2: Crawling/Scraping data from the Web (entire websites).\n",
    "\n",
    "TOPIC3: Getting data via APIs (JSON format)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOPIC1: Getting data from a Web URL: text, HTML, PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luthe\\miniconda3\\envs\\comp47350py311\\python.exe\n"
     ]
    }
   ],
   "source": [
    "#To check which Python version and virtual environment this Jupyter Notebook uses\n",
    "print(sys.executable)\n",
    "#print(sys.version_info)\n",
    "#print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all required packages\n",
    "#If you don't have these packages, install using: pip install <package-name>\n",
    "\n",
    "#Import package 'requests'for URL scrapping\n",
    "import requests\n",
    "# Import package for reading csv files \n",
    "import pandas as pd\n",
    "#import package 'beautifulsoup' to extract the content of HTML fields \n",
    "#pip install bs4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#pip install newspaper3k\n",
    "import newspaper\n",
    "\n",
    "#import package 'feedparser'\n",
    "#Feedparser is a library to parse RSS/XML feeds, these are files with a specific XML structure\n",
    "import feedparser\n",
    "#import package 'json' to parse json objects\n",
    "import json\n",
    "\n",
    "import time\n",
    "\n",
    "#Look at the package structure to understand how to use it\n",
    "# print(dir(requests))\n",
    "\n",
    "#Look at individual functions\n",
    "# help(requests.get)\n",
    "\n",
    "#As an alternative can use '?', same as help() but opens a new window\n",
    "#?requests.get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** START OF THE PROJECT GUTENBERG EBOOK 11 ***\n",
      "[Illustration]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Alice’s Adventures in Wonderland\n",
      "\n",
      "by Lewis Carroll\n",
      "\n",
      "THE MILLENNIUM FULCRUM EDITION 3.0\n",
      "\n",
      "Contents\n",
      "\n",
      " CHAPTER I.     Down the Rabbit-Hole\n",
      " CHAPTER II.    The Pool of Tears\n",
      " CHAPTER III.   A Caucus-Race and a Long Tale\n",
      " CHAPTER IV.    The Rabbit Sends in a Little Bill\n",
      " CHAPTER V.     Advice from a Caterpillar\n",
      " CHAPTER VI.    Pig and Pepper\n",
      " CHAPTER VII.   A Mad Tea-Party\n",
      " CHAPTER VIII.  The Queen’s Croquet-Ground\n"
     ]
    }
   ],
   "source": [
    "#Get a text file.\n",
    "#Get book \"Alice's Adventures in Wonderland\" from Project Gutenberg, in text format\n",
    "\n",
    "#Give the URL for the file to be downloaded\n",
    "url='https://www.gutenberg.org/files/11/11-0.txt'\n",
    "#Look at the object returned by requests.get()\n",
    "requests_object = requests.get(url)\n",
    "\n",
    "#print(requests_object.content)\n",
    "\n",
    "#Get the content from the downloaded text file\n",
    "text_page = requests_object.text\n",
    "#print(text_page)\n",
    "\n",
    "#Look at the first 500 characters of the book\n",
    "print(text_page[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Reading from a csv file, into a data frame\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMotorInsuranceFraudClaimABTFull.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Check how many rows and columns this dataframe has\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber of rows and columns:\u001b[39m\u001b[38;5;124m\"\u001b[39m, df\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Reading from a csv file, into a data frame\n",
    "df = pd.read_csv('MotorInsuranceFraudClaimABTFull.csv')\n",
    "\n",
    "# Check how many rows and columns this dataframe has\n",
    "print(\"number of rows and columns:\", df.shape)\n",
    "\n",
    "# Show first 10 rows of data frame\n",
    "# The rows are indexed starting from 0\n",
    "df\n",
    "\n",
    "# Show last 10 rows of data frame\n",
    "# The rows are indexed starting from 0\n",
    "#df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.irishtimes.com/ireland/2024/01/16/low-temperature-warning-issued-for-entire-country-by-met-eireann-with-snow-warning-for-several-counties/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#Get the content from the downloaded html file\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m html_page \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241m.\u001b[39mget(url)\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#Look at the format of the html file\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(html_page[:\u001b[38;5;241m500\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "#Get an HTML file.\n",
    "#Get news article from IrishTimes website.\n",
    "\n",
    "#Give the URL for the file to be downloaded\n",
    "url = \"https://www.irishtimes.com/ireland/2024/01/16/low-temperature-warning-issued-for-entire-country-by-met-eireann-with-snow-warning-for-several-counties/\"\n",
    "\n",
    "#Get the content from the downloaded html file\n",
    "html_page = requests.get(url).text\n",
    "#Look at the format of the html file\n",
    "print(html_page[:500])\n",
    "\n",
    "#write the content to a file\n",
    "file = open(\"low-temperature-warning-issued-for-entire-country.html\", \"w\") \n",
    "file.write(html_page)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authors: ['Jennifer Bray', 'Jennifer Bray Is A Political Correspondent With The Irish Times', 'Pat Leahy', 'Pat Leahy Is Political Editor Of The Irish Times', 'Jack Horgan-Jones', 'Jack Horgan-Jones Is A Political Correspondent With The Irish Times', 'Fri Jan -']\n",
      "Date: 2025-01-23 00:00:00\n",
      "Title: New Government pledges to go on offensive over US trade\n",
      "Text: The newly formed Government has pledged an immediate trade offensive in the United States, with Taoiseach Micheál Martin also promising fresh reforms in housing, justice and climate.\n",
      "\n",
      "A new Cabinet made up of 15 Fianna Fáil and Fine Gael Ministers was appointed by Mr Martin on Thursday evening after the Dáil voted by 95 votes to 76 to elect the Fianna Fáil leader as Taoiseach for a second time.\n",
      "\n",
      "Tánaiste Simon Harris, who was appointed as Minister for Foreign Affairs and Trade, immediately moved to convene a trade conference of all Irish consulates and embassies in the United States.\n",
      "\n",
      "Mr Harris said this is “with the sole focus of targeting the thousands of decision-makers in the Trump administration and ensuring they know the two-way relationship and strength of Irish investment in the United States”. The entire trade division led by an assistant secretary general will move to Iveagh House.\n",
      "\n",
      "READ MORE\n",
      "\n",
      "Mr Martin pledged to develop “a new relationship” with the United Kingdom that will see “regular, formal summits” held while all Ministers will be tasked with building fresh bilateral relations.\n",
      "\n",
      "The newly appointed Fianna Fáil Minister for Justice, Jim O’Callaghan, will lead efforts to reform the department, with a new division due to “take responsibility for the control of our borders”, Mr Martin said. This will be headed up by a secretary general with an individual budget and more direct oversight. On housing, a co-ordination and implementation group will operate from the Department of the Taoiseach while a new “climate clearing house” will also be established by Mr Martin.\n",
      "\n",
      "There was strong criticism of the make-up of the new Cabinet, however, with only three women appointed to senior ministries. Sinn Féin leader Mary Lou McDonald told Mr Martin in the Dáil that “you’ve got the gender balance wrong. You’re very male, some of you are pale, but you are very male.”\n",
      "\n",
      "Labour leader Ivana Bacik also criticised the make-up of the Cabinet, while Social Democrats TD Jennifer Whitmore said “there are now as many men named James in the Cabinet as there are women”. The Women For Election group said it was “shocking and disappointing”.\n",
      "\n",
      "There were few surprises on the Fine Gael side, as Longford-Westmeath TD Peter Burke was the only Minister to hold on to his existing brief, at Enterprise. Dún Laoghaire TD Jennifer Carroll MacNeill was promoted to Minister for Health, while Kildare South TD Martin Heydon was promoted to Minister for Agriculture. Fine Gael’s Paschal Donohoe swapped places with Fianna Fáil Minister for Finance Jack Chambers, who took the Department of Public Expenditure.\n",
      "\n",
      "Fine Gael deputy leader Helen McEntee is the new Minister for Education, while Patrick O’Donovan is now the Minister for Arts, Media, Communications, Culture and Sport.\n",
      "\n",
      "In Fianna Fáil, the biggest change was for outgoing Minister for Housing Darragh O’Brien, who will move over to the Department of Transport, Environment and Climate Change with Wexford TD James Browne becoming the new Minister for Housing. Mayo TD Dara Calleary saw off competition from other senior Fianna Fáil TDs to become the new Minister for Social Protection, with Waterford TD Mary Butler – who had been tipped as a contender for a senior ministry – becoming the new Chief Whip. Norma Foley is the new Minister for Children, Disability and Equality. There was also a promotion for Kildare North TD James Lawless, as he becomes Minister for Higher Education.\n",
      "\n",
      "The process of electing a Taoiseach finally got under way on Thursday after it was agreed that the Regional Independent Group, who support the Government, would not be given Opposition speaking time in a new technical group. Although Labour TD Alan Kelly said the agreement was “iron clad”, the matter will now be considered by a new Dáil reform committee.\n",
      "\n",
      "The Regional Independent Group also described the solution as a “temporary measure”, in a sign that the matter may still prove contentious.\n",
      "\n",
      "URL: https://www.irishtimes.com/politics/2025/01/23/new-government-pledges-to-go-on-offensive-over-us-trade-alongside-reforms-in-housing-justice-and-climate/\n"
     ]
    }
   ],
   "source": [
    "# we can download a newsarticle and parse it using the newspaper3k library\n",
    "# https://buildmedia.readthedocs.org/media/pdf/newspaper/latest/newspaper.pdf\n",
    "# newspaper cannot parse all types of html files, for more complex file structure we still need 'beautifulsoup'\n",
    "from newspaper import Article\n",
    "\n",
    "url =\"https://www.irishtimes.com/politics/2025/01/23/new-government-pledges-to-go-on-offensive-over-us-trade-alongside-reforms-in-housing-justice-and-climate/\"\n",
    "article = Article(url)\n",
    "article.download()\n",
    "\n",
    "#print(article.html)\n",
    "\n",
    "article.parse()\n",
    "print(\"Authors:\", article.authors)\n",
    "print(\"Date:\", article.publish_date)\n",
    "print(\"Title:\", article.title)\n",
    "print(\"Text:\", article.text)\n",
    "print(\"\\nURL:\", article.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?><rss xmlns:atom=\"http://www.w3.org/2005/Atom\" xmlns:content=\"http://purl.org/rss/1.0/modules/content/\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:sy=\"http://purl.org/rss/1.0/modules/syndication/\" version=\"2.0\" xmlns:media=\"http://search.yahoo.com/mrss/\"><channel><title><![CDATA[Irish Times Feeds]]></title><link>https://www.irishtimes.com</link><atom:link href=\"https://www.irishtimes.com/arc/outboundfeeds/digest-resolver/homepage-top/\" rel=\"self\" type=\"application/rss+xml\"/><description><![CDATA[Irish Times Feeds News Feed]]></description><lastBuildDate>Thu, 30 Jan 2025 13:14:41 +0000</lastBuildDate><language>en</language><ttl>1</ttl><sy:updatePeriod>hourly</sy:updatePeriod><sy:updateFrequency>1</sy:updateFrequency><item><title><![CDATA[Washington DC plane crash: ‘No survivors’ expected after American Airlines flight collided with helicopter]]></title><link>https://www.irishtimes.com/world/us/2025/01/30/washington-dc-plane-crash-live-updates/\n",
      "https://www.irishtimes.com/world/us/2025/01/30/washington-dc-plane-crash-live-updates/\n",
      "\n",
      "Article title: Washington DC plane crash: ‘No survivors’ expected after American Airlines flight collided with helicopter \n",
      "\n",
      "\n",
      "Article first paragraph: Main points \n",
      "\n",
      "https://www.irishtimes.com/health/2025/01/30/vhi-hikes-health-insurance-premiums-by-average-of-3/\n",
      "\n",
      "Article title: VHI hikes health insurance premiums by average of 3% \n",
      "\n",
      "\n",
      "Article first paragraph: VHI Healthcare has announced an average price increase of 3 per cent across its health insurance plans effective from the beginning of March. \n",
      "\n",
      "https://www.irishtimes.com/sport/rugby/2025/01/30/ireland-name-team-to-play-england-live-updates-as-sam-prendergast-picked-to-start-at-10-for-six-nations-opener/\n",
      "\n",
      "Article title: Ireland name team to play England: Live updates as Sam Prendergast picked to start at 10 for Six Nations opener \n",
      "\n",
      "\n",
      "Article first paragraph: IRELAND TEAM TO FACE ENGLAND \n",
      "\n",
      "https://www.irishtimes.com/food/restaurants/review/2025/01/30/reggies-pizzeria-review-the-margherita-is-a-study-in-simplicity-at-this-ambitious-new-rathmines-venture/\n",
      "\n",
      "Article title: Reggie’s Pizzeria review: The Margherita is a study in simplicity at this ambitious new Rathmines venture \n",
      "\n",
      "\n",
      "Article first paragraph: Reggie's Pizzeria      Address : 221/223 Rathmines Rd Lower, Rathmines, Dublin 6, D06 A582. Telephone : N/A Cuisine : Italian Website : https://www.reggies.ie/ Opens in new window Cost : €€ \n",
      "\n",
      "https://www.irishtimes.com/your-money/2025/01/30/more-than-200-customers-paid-deposits-to-dublin-window-firm-but-are-unlikely-to-get-refunds-warn-liquidators/\n",
      "\n",
      "Article title: Hundreds of Dublin window firm customers, many with deposits over €10,000, unlikely to get refunds \n",
      "\n",
      "\n",
      "Article first paragraph: More than 200 customers of a Dublin window and doors company that closed suddenly at the end of last November are now facing significant financial losses, says a preliminary report issued by liquidators this week. \n",
      "\n",
      "https://www.irishtimes.com/culture/tv-radio/2025/01/30/patrick-freyne-oh-no-not-these-guys-again-every-year-irelands-fittest-family-torment-the-nation/\n",
      "\n",
      "Article title: Oh no, not these guys again! Every year Ireland’s fittest family torment the nation – The Irish Times \n",
      "\n",
      "\n",
      "Article first paragraph: Oh no, not these guys again! Every year Ireland’s fittest family torment the nation as they march solidly into the future with their straight backs and open hearts. They make you cast a resentful glance at your own inferior family. I mean, the absolute state of them. \n",
      "\n",
      "https://www.irishtimes.com/your-money/2025/01/30/two-weeks-on-the-governments-housing-targets-are-already-in-tatters/\n",
      "\n",
      "Article title: Two weeks on, the Government’s housing targets are already in tatters \n",
      "\n",
      "\n",
      "Article first paragraph: The new Government got two pieces of bad news on housing in its first days in office. The first were figures showing that housing completions last year were 30,330, well below the close to 40,000 which senior ministers had forecast during the election campaign, despite analysts saying this was not possible. \n",
      "\n",
      "https://www.irishtimes.com/world/middle-east/2025/01/30/palestinian-militants-begin-handover-of-three-israeli-hostages-in-latest-stage-of-ceasefire-deal/\n",
      "\n",
      "Article title: Palestinian militants begin handover of three Israeli hostages in latest stage of ceasefire deal \n",
      "\n",
      "\n",
      "Article first paragraph: Militants in Gaza have agreed to release three Israeli hostages, including soldier Agam Berger (20), in exchange for 110 Palestinian prisoners. Video: Anadolu \n",
      "\n",
      "https://www.irishtimes.com/culture/2025/01/30/festivals-in-ireland-2025-from-longitude-to-all-together-now-a-guide-to-80-of-the-best/\n",
      "\n",
      "Article title: Festivals in Ireland 2025: From Longitude to All Together Now - a guide to 80 of the best \n",
      "\n",
      "\n",
      "Article first paragraph: From heavy hitters such as Electric Picnic and Dublin Theatre Festival to more esoteric gatherings, the Irish festival calendar has something for everyone. The list below offers just a taste of the cultural showcases coming up in 2025. Lots of festivals won’t announce their programmes until later in the year, so keep an eye on their websites and social media. \n",
      "\n",
      "https://www.irishtimes.com/opinion/2025/01/30/michael-d-higgins-is-expending-the-goodwill-that-was-once-held-in-huge-reserves-for-ireland/\n",
      "\n",
      "Article title: I am tired of explaining Michael D Higgins’s words to incredulous English people – The Irish Times \n",
      "\n",
      "\n",
      "Article first paragraph: “Why couldn’t he focus on the Holocaust for just one day?” a WhatsApp message from a friend, a UK-based journalist, read on Tuesday. Others on the group chat agreed with the tenor of the question. I realised how weary I had grown of trying to explain President Michael D Higgins’s words and proclivities to curious but incredulous English people. The first time I remember doing so is when he wrote an article in the Guardian in 2021, on the centenary of partition, in which he lectured Britain about its imperial past. Hosting a letter about brokering peace with Russia, written by his wife, on his presidential website provoked some raised eyebrows too. \n",
      "\n",
      "https://www.irishtimes.com/culture/tv-radio/2025/01/30/russell-brand-bbc-apologises-after-staff-felt-unable-to-raise-complaints-about-presenter/\n",
      "\n",
      "Article title: Russell Brand: BBC apologises after staff felt ‘unable to raise’ complaints about presenter \n",
      "\n",
      "\n",
      "Article first paragraph: Russell Brand: denies the accusations, and previously said all his sexual relationships were 'absolutely always consensual'. Photograph: Collins Photos \n",
      "\n",
      "https://www.irishtimes.com/politics/oireachtas/2025/01/29/three-outgoing-nui-senators-expected-to-retain-their-seats/\n",
      "\n",
      "Article title: NUI and Trinity Seanad election counts: Michael McDowell is elected and Lynn Ruane tops poll \n",
      "\n",
      "\n",
      "Article first paragraph: The ornate setting of Exam Hall at Trinity College Dublin where the counting of votes for the 2025 University of Dublin Seanad Election is taking place. Photograph: Bryan O’Brien / The Irish Times. \n",
      "\n",
      "https://www.irishtimes.com/culture/tv-radio/2025/01/29/brian-and-maggie-review-the-interview-that-led-to-thatchers-downfall-has-lessons-for-ireland/\n",
      "\n",
      "Article title: Brian and Maggie review: Why does no Irish broadcaster bring our history to light in such a riveting way? \n",
      "\n",
      "\n",
      "Article first paragraph: While Brian Walden is thoroughly obscure in Ireland, in the UK, the former British Labour Party MP is regarded as one of the finest practitioners of the lost art of the long-form political interview. He is particularly famous/notorious for a bruising 1989 London Weekend Television grilling of the then British prime minister Margaret Thatcher when he pressed her about her intolerance for opposing views. \n",
      "\n",
      "https://www.irishtimes.com/ireland/education/2025/01/30/three-quarters-of-schools-had-no-applicants-for-recent-teaching-vacancies-survey-finds/\n",
      "\n",
      "Article title: No applicants for teaching jobs at 75% of schools with recent vacancies, survey finds \n",
      "\n",
      "\n",
      "Article first paragraph: Teacher vacancies: More than 40 per cent of schools have had to limit the access of students to particular subjects because of a shortage of teachers in these areas, the survey found. Photograph: iStock \n",
      "\n",
      "https://www.irishtimes.com/your-money/2025/01/30/luke-was-left-waiting-five-months-for-a-part-for-his-off-the-road-opel-with-no-joy-and-then-he-contacted-us/\n",
      "\n",
      "Article title: A new part for an Opel Astra left this reader off the road for five months \n",
      "\n",
      "\n",
      "Article first paragraph: 'I brought my car to a main Opel dealer with the reasonable expectation of professional service, timely repairs, and honest communication.' Photograph: Ronald Wittek/EPA \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Downloading and working with an XML file\n",
    "#Get the whole RSS feed for the Irish Times news articles\n",
    "#This is an XML file listing the URLs of individual news articles published online\n",
    "#Need to know the structure of the XML to be able to extract text from specific tags\n",
    "\n",
    "#Parse the XML file to retrieve the URLs for individual news articles.\n",
    "#Parse each article's HTML page\n",
    "\n",
    "def getArticleDetailsByUrl(url):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "\n",
    "    #print(article.html)\n",
    "    article.parse()\n",
    "    authors = article.authors\n",
    "    date = article.publish_date\n",
    "    title = article.title\n",
    "    text = article.text\n",
    "\n",
    "    return [authors, date, title, text]\n",
    "\n",
    "def scrapeRSSFeed(rss_feed):\n",
    "    d = feedparser.parse(rss_feed)\n",
    "    #print(d)\n",
    "    #print(d['entries'], \"\\n\")\n",
    "        \n",
    "    for item in d['entries']:\n",
    "        #Extract an article URL\n",
    "        article_url = item['link']\n",
    "        print(article_url)\n",
    "        try:\n",
    "            [authors, date, title, text] = getArticleDetailsByUrl(article_url)\n",
    "            print(\"\\nArticle title:\", title, \"\\n\")\n",
    "            print(\"\\nArticle first paragraph:\", text.split(\"\\n\")[0], \"\\n\")\n",
    "            #we introduce a delay after each article download to avoid overloading the IrishTimes server\n",
    "            time.sleep(5)\n",
    "        except requests.RequestException as e:\n",
    "            print(\"[Error]: \" + str(e))\n",
    "            \n",
    "#Here you have your very own RSS feed reader in a few lines of code.\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #The URL of the XML file\n",
    "    url='https://www.irishtimes.com/rss/irish-times-top-10-stories-1.4019566'\n",
    "    xml_page = requests.get(url).text\n",
    "    \n",
    "    #Look at the structure of the XML file\n",
    "    #To have a proper look, open the XML file with a text editor\n",
    "    print(xml_page[:1000])\n",
    "\n",
    "    # Call the method that parses a given XML file\n",
    "    scrapeRSSFeed(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'%PDF-1.5\\n%\\xd0\\xd4\\xc5\\xd8\\n2 0 obj <<\\n/Type /ObjStm\\n/N 100\\n/First 804\\n/Length 1113      \\n/Filter /FlateDecode\\n>>\\nstream\\nx\\xda\\x9dV\\xdbn\\x9cH\\x10}\\x9f\\xaf\\xa8\\xc7d\\xb5\\x8a\\xe9\\x0b\\xdd\\xb0\\x8a\\x12E\\x9b8\\xca\\xc3*Vl%\\xcf\\x1d\\xe8\\x19\\xa30\\x80\\x1a\\xb0=\\xfb\\xf5{\\x8a\\x8b\\xed\\xec\\xa5\\x07\\xed\\x83M\\x0f\\xd49UuNu\\x83\\xa0\\x84RR\\x82\\x0ciA9YA\"\\xa1\\xdc\\x90P$dFB\\x930\\x92\\x84%\\x91c\\x99\\x91\\x14)\\xfeHj\\xbb\\x93\\x92\\xa4\\xc1\\x1d\\xe0\\x93\\x04KR\\xe0\\x919)\\xa3p\\x87T\\x86\\x8b\\x02-\\x9ek\\xd2\\xd2\\x92\\xb2\\xa4S<\\xcfH\\xdb\\x84\\xf3\\xa5\\x89\\xdciI)\\xb2\\xe8\\x94R\\x8d\\x8b\\xa14\\xc3%\\'#\\xb0L\\xc8(<Pd\\x8c\\xc5s26\\xa7\\xd4\\xa2J`3\\xb2\\xa00\\x82\\xacU;\\xd4h\\xf3\\x8cLJ\\x19R\\x1bC\\x19\\x12\\x99\\x9c\\xb2\\x1c\\xcf\\xd1\\x10\\xaa\\xb3\\x8a\\xf2\\tDy\\x96\\x00\\x84F\\x05P\\xe81Q\\x922n\\xdc\\xe8]\\x86f\\x13\\x0b\\x9a\\x94\\x84H2\\xca \\x85\\x90\\t\\x88pE!y\\x82\\xab\\x01\\x07\\xf4\\x11\\xb9%VE&9a)\\xa4\\xc2\\x15|2e2\\xc1\\x02\\xaa\\x9dH\\xc0\\xc8\\x02\\x89\\x04\\x94J1?\\xcbk\\xf8\\x0eHU.9\\x13\\x94\\xe6_,\\xbbF[\\x82\\x85\\xd7)\\x17\\x01b\\x9d\\xe1\\x1f\\xc4\\x17)\\x07B~\\x91j\\xb9\\x130@\\xa4h\\x19\\t\\xb1\\x98*\\x00\\xb3a\\x04l@\\x95L\\x0f\\x80\\xb1\\x9c\\x10\\xcc\\x16\\r\\t\\x98!\\xac\\xe6Z\\x10g\\xb9\\x7f\\x18\"l\\x8e;\\x9c8\\x93\\xd9N\\xc0\\x14\\x91\\x19\\xb6\\x1fwsn\\x05\\xc6\\x88\\x9cK`\\xd2\\x9c\\x1b\\x849\\x10\\x00\\xacPE&\\xdc\\x13\\x0c\\x92\\xc9T\\xa6\\xc5b\\x8a\\xc1x$<'\n"
     ]
    }
   ],
   "source": [
    "#Get a PDF file, save it to disk.\n",
    "\n",
    "# Give url of the PDF file\n",
    "url='http://www.greenteapress.com/thinkpython/thinkpython.pdf'\n",
    "# Download the pdf file into request_object\n",
    "request_object = requests.get(url)\n",
    "\n",
    "#PDF is a binary format. Use request.content instead of request.text\n",
    "#Write binary content on your machine's disk in a file named 'thinkpython.pdf'\n",
    "with open(\"thinkpython.pdf\", \"wb\") as pdffile:\n",
    "    # Look at the conent of the file; it looks all gibberish since it is a binary format.\n",
    "    # To make sense of the content, we need tools that can read pdf format and extract it to plain text.\n",
    "    # See next cell for pdftotext tool.\n",
    "    print(request_object.content[:500])\n",
    "    \n",
    "    #Print the content of the request_object to a file named \"thinkpython.pdf\"\n",
    "    pdffile.write(request_object.content)\n",
    "\n",
    "#Check that it downloaded the file to the current directory.\n",
    "#%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pages: 240\n",
      "Page 0:\n",
      " Think Python\n",
      "How to Think Like a Computer Scientist\n",
      "\n",
      "Version 2.0.17\n",
      "\n",
      "\f",
      "\n",
      "Page 1:\n",
      " \f",
      "\n",
      "\n",
      "\n",
      "The first 500 symbols in the string:\n",
      " Think Python\n",
      "How to Think Like a Computer Scientist\n",
      "\n",
      "Version 2.0.17\n",
      "\n",
      "\f",
      "\n",
      "\n",
      "\f",
      "\n",
      "\n",
      "Think Python\n",
      "How to Think Like a Computer Scientist\n",
      "\n",
      "Version 2.0.17\n",
      "\n",
      "Allen Downey\n",
      "\n",
      "Green Tea Press\n",
      "Needham, Massachusetts\n",
      "\n",
      "\f",
      "\n",
      "\n",
      "Copyright © 2012 Allen Downey.\n",
      "Green Tea Press\n",
      "9 Washburn Ave\n",
      "Needham MA 02492\n",
      "Permission is granted to copy, distribute, and/or modify this document under the terms of the\n",
      "Creative Commons Attribution-NonCommercial 3.0 Unported License, which is available at http:\n",
      "//creativeco\n"
     ]
    }
   ],
   "source": [
    "import pdftotext\n",
    "\n",
    "# Make sure you have downloaded the \"thinkpython.pdf\" file in your current folder\n",
    "# http://www.greenteapress.com/thinkpython/thinkpython.pdf\n",
    "\n",
    "# Load your PDF\n",
    "with open('thinkpython.pdf', 'rb') as f:\n",
    "    pdf = pdftotext.PDF(f)\n",
    "    \n",
    "# What kind of object is this?\n",
    "#print(type(pdf)\n",
    "\n",
    "# What are the methods and variables of this object.\n",
    "#print(dir(pdf))\n",
    "\n",
    "# Get more detail about how to use this object\n",
    "# print(help(pdf))\n",
    "\n",
    "# How many pages?\n",
    "print(\"pages:\", len(pdf))\n",
    "\n",
    "# Iterate over all the pages\n",
    "#for page in pdf:\n",
    "#    print(\"\\n=====newpage:=====\\n\", page)\n",
    "\n",
    "# Read some individual pages\n",
    "print(\"Page 0:\\n\", pdf[0])\n",
    "print(\"Page 1:\\n\", pdf[1])\n",
    "\n",
    "# Read all the text into one string\n",
    "string_pdf = \"\\n\\n\".join(pdf)\n",
    "\n",
    "# Print the first 500 characters in the string\n",
    "print(\"\\n\\nThe first 500 symbols in the string:\\n\", string_pdf[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic2: Crawling data from the Web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative to using the Python package *requests*, you can use the command line *wget* utility to download an HTML page from a given URL or to download an entire website. If you don't have *wget* on your computer, first install it for your platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *wget* tool is great for crawling entire or parts of websites. It recursively follows URLs up to given depth.\n",
    "The example below downloads a part of the website locally, in a folder named *en.wikipedia.org*. The parameter -l tells wget to what depth it should follow URLs from the original URL. The parameter --no-parent tells wget to not download anything other than the given path. See http://linuxreviews.org/quicktips/wget/ for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "#Crawl the website to depth 1. To stop downloading interrupt the kernel from the menu above.\n",
    "! wget -r -l 1 --no-parent https://en.wikipedia.org/wiki/Main_Page "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-30 13:16:30--  http://www.kdnuggets.com/\n",
      "Resolving www.kdnuggets.com (www.kdnuggets.com)... 104.26.3.64, 172.67.68.178, 104.26.2.64\n",
      "Connecting to www.kdnuggets.com (www.kdnuggets.com)|104.26.3.64|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://www.kdnuggets.com/ [following]\n",
      "--2025-01-30 13:16:30--  https://www.kdnuggets.com/\n",
      "Connecting to www.kdnuggets.com (www.kdnuggets.com)|104.26.3.64|:443... connected.\n",
      "HTTP request sent, awaiting response... 403 Forbidden\n",
      "2025-01-30 13:16:30 ERROR 403: Forbidden.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Need to stop crawling after a short while, otherwise it may fill your hard disk or you will get banned by the website\n",
    "! wget -E -p -l 1 --no-parent http://www.kdnuggets.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a pure Python crawler we can use the Python *wget* package or the *scrapy* package (scrapy only works with Phyton2.7 though). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic3: Getting data via APIs.\n",
    "### JSON format: \n",
    "JavaScript Object Notation - a text format used widely for web-based resource sharing. Many packages and APIs return data in JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a file named *example.json* using the Python code below to write a given string to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_string = \"\"\"\n",
    "{\n",
    "    \"glossary\": {\n",
    "        \"title\": \"example glossary\",\n",
    "\t\t\"GlossDiv\": {\n",
    "            \"title\": \"S\",\n",
    "\t\t\t\"GlossList\": {\n",
    "                \"GlossEntry\": {\n",
    "                    \"ID\": \"SGML\",\n",
    "\t\t\t\t\t\"SortAs\": \"SGML\",\n",
    "\t\t\t\t\t\"GlossTerm\": \"Standard Generalized Markup Language\",\n",
    "\t\t\t\t\t\"Acronym\": \"SGML\",\n",
    "\t\t\t\t\t\"Abbrev\": \"ISO 8879:1986\",\n",
    "\t\t\t\t\t\"GlossDef\": {\n",
    "                        \"para\": \"A meta-markup language, used to create markup languages such as DocBook.\",\n",
    "\t\t\t\t\t\t\"GlossSeeAlso\": [\"GML\", \"XML\"]\n",
    "                    },\n",
    "\t\t\t\t\t\"GlossSee\": \"markup\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\"\"\"\n",
    "with open(\"example.json\", \"w\") as file:\n",
    "    file.write(json_string)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "    \"glossary\": {\n",
      "        \"title\": \"example glossary\",\n",
      "\t\t\"GlossDiv\": {\n",
      "            \"title\": \"S\",\n",
      "\t\t\t\"GlossList\": {\n",
      "                \"GlossEntry\": {\n",
      "                    \"ID\": \"SGML\",\n",
      "\t\t\t\t\t\"SortAs\": \"SGML\",\n",
      "\t\t\t\t\t\"GlossTerm\": \"Standard Generalized Markup Language\",\n",
      "\t\t\t\t\t\"Acronym\": \"SGML\",\n",
      "\t\t\t\t\t\"Abbrev\": \"ISO 8879:1986\",\n",
      "\t\t\t\t\t\"GlossDef\": {\n",
      "                        \"para\": \"A meta-markup language, used to create markup languages such as DocBook.\",\n",
      "\t\t\t\t\t\t\"GlossSeeAlso\": [\"GML\", \"XML\"]\n",
      "                    },\n",
      "\t\t\t\t\t\"GlossSee\": \"markup\"\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}"
     ]
    }
   ],
   "source": [
    "# Run command \"cat\" to look at the file\n",
    "# The sign ! tells Jupyter Notebook that the following is a Terminal command.\n",
    "!cat example.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'glossary': {'title': 'example glossary', 'GlossDiv': {'title': 'S', 'GlossList': {'GlossEntry': {'ID': 'SGML', 'SortAs': 'SGML', 'GlossTerm': 'Standard Generalized Markup Language', 'Acronym': 'SGML', 'Abbrev': 'ISO 8879:1986', 'GlossDef': {'para': 'A meta-markup language, used to create markup languages such as DocBook.', 'GlossSeeAlso': ['GML', 'XML']}, 'GlossSee': 'markup'}}}}}\n"
     ]
    }
   ],
   "source": [
    "json_data = json.load(open('example.json'))\n",
    "#json_data looks like a nested Python dictionary\n",
    "print(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example glossary\n",
      "S\n",
      "SGML\n"
     ]
    }
   ],
   "source": [
    "#We can refer to different fields of the json object\n",
    "print(json_data['glossary']['title'])\n",
    "print(json_data['glossary']['GlossDiv']['title'])\n",
    "print(json_data['glossary']['GlossDiv']['GlossList']['GlossEntry']['ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below we use an URL called an API endpoint and the *requests* package to get a json file, as we have seen above in getting data from an URL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "476875\n",
      "[{\"entityid\":\"18691000001\",\"entityname\":\"DENVER GAS COMPANY, Dissolved November 30, 1874\",\"entitystatus\":\"Administratively Dissolved\",\"jurisdictonofformation\":\"CO\",\"entitytype\":\"DPC\",\"entityformdate\":\"1869-11-13T00:00:00.000\"}\n",
      ",{\"entityid\":\"18751000001\",\"entityname\":\"SILVER CLOUD MINING COMPANY, Dissolved January 1, 1880\",\"entitystatus\":\"Administratively Dissolved\",\"jurisdictonofformation\":\"CO\",\"entitytype\":\"DPC\",\"entityformdate\":\"1875-12-06T00:00:00.000\"}\n",
      ",{\"entityid\":\"18811004244\",\"entityname\"\n"
     ]
    }
   ],
   "source": [
    "url='https://data.colorado.gov/resource/4ykn-tg5h.json'\n",
    "json_dataset = requests.get(url).text\n",
    "print(len(json_dataset))\n",
    "#Look at the first 500 characters of the json list\n",
    "print(json_dataset[:500])\n",
    "\n",
    "with open(\"data_colorado_gov.json\", \"w\") as file:\n",
    "    file.write(json_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
